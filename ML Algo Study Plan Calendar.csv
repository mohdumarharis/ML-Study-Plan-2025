Subject,Start Date,Start Time,End Date,End Time,Description,Location
"Study Linear Algebra",07/26/2025,19:00,07/26/2025,21:00,"Read 'Mathematics for Machine Learning' Ch. 1 (vectors, matrices). Watch 3Blue1Brown 'Vectors'.","Online"
"Code Matrix Operations",07/27/2025,19:00,07/27/2025,21:30,"Study matrix operations, determinants. Solve exercises (Khan Academy).","Online"
"Study Calculus",07/28/2025,19:00,07/28/2025,21:30,"Read Ch. 2, study derivatives. Watch 3Blue1Brown 'Derivatives'.","Online"
"Study Probability",07/29/2025,19:00,07/29/2025,21:30,"Read Ch. 3, focus on Bayes’ theorem. Solve probability exercises.","Online"
"Solve Math Exercises",07/30/2025,19:00,07/30/2025,21:30,"Code matrix multiplication, inverse in NumPy. Solve linear algebra exercises.","Online"
"Review Math",07/31/2025,19:00,07/31/2025,21:30,"Review weak areas, revisit Khan Academy problems.","Online"
"Study Linear Regression",08/02/2025,19:00,08/02/2025,21:30,"Read ISLR Ch. 3, derive Normal Equation. Watch Andrew Ng Week 1.","Online"
"Study Gradient Descent",08/03/2025,19:00,08/03/2025,21:30,"Study Gradient Descent, derive update rule. Watch Andrew Ng.","Online"
"Code Linear Regression",08/04/2025,19:00,08/04/2025,21:30,"Implement Linear Regression from scratch (NumPy). Ref: Scikit-learn docs.","Online"
"Apply Linear Regression",08/05/2025,19:00,08/05/2025,21:30,"Apply to Boston Housing dataset (scikit-learn). Plot predictions.","Online"
"Study Ridge",08/06/2025,19:00,08/06/2025,21:30,"Study Ridge regularization, derive L2 penalty. Apply Ridge.","Online"
"Compare Models",08/07/2025,19:00,08/07/2025,21:30,"Compare Linear vs. Ridge on Boston Housing. Document results.","Online"
"Study Logistic Regression",08/09/2025,19:00,08/09/2025,21:30,"Read ISLR Ch. 4, derive cross-entropy loss. Watch StatQuest.","Online"
"Code Logistic Regression",08/10/2025,19:00,08/10/2025,21:30,"Implement Logistic Regression from scratch (NumPy).","Online"
"Study Naive Bayes",08/11/2025,19:00,08/11/2025,21:30,"Read about Naive Bayes, derive Gaussian variant. Watch StatQuest.","Online"
"Code Naive Bayes",08/12/2025,19:00,08/12/2025,21:30,"Implement Gaussian Naive Bayes from scratch.","Online"
"Apply to Iris",08/13/2025,19:00,08/13/2025,21:30,"Apply Logistic Regression and Naive Bayes to Iris dataset.","Online"
"Analyze Iris Results",08/14/2025,19:00,08/14/2025,21:30,"Analyze results, plot decision boundaries.","Online"
"Study Regression Metrics",08/16/2025,19:00,08/16/2025,21:30,"Read ISLR Ch. 5, study MSE, RMSE, R².","Online"
"Study Classification Metrics",08/17/2025,19:00,08/17/2025,21:30,"Study Accuracy, F1, ROC-AUC. Watch Kaggle tutorial.","Online"
"Evaluate Models",08/18/2025,19:00,08/18/2025,21:30,"Evaluate Week 2–3 models with metrics (scikit-learn).","Online"
"Mini-Project: Titanic",08/19/2025,19:00,08/19/2025,21:30,"Apply Linear and Logistic Regression to Titanic dataset.","Online"
"Mini-Project: Compare",08/20/2025,19:00,08/20/2025,21:30,"Compare models, plot ROC curves.","Online"
"Mini-Project: Document",08/21/2025,19:00,08/21/2025,21:30,"Document findings, write summary.","Online"
"Study Decision Trees",08/23/2025,19:00,08/23/2025,21:30,"Read ISLR Ch. 8, derive Entropy. Ref: Géron Ch. 6.","Online"
"Code Decision Tree",08/24/2025,19:00,08/24/2025,21:30,"Implement Decision Tree from scratch.","Online"
"Study Random Forests",08/25/2025,19:00,08/25/2025,21:30,"Study Random Forests, bagging. Read Géron.","Online"
"Apply Random Forest",08/26/2025,19:00,08/26/2025,21:30,"Apply Random Forest to Wine dataset (scikit-learn).","Online"
"Tune Random Forest",08/27/2025,19:00,08/27/2025,21:30,"Tune n_estimators, max_depth.","Online"
"Compare Trees",08/28/2025,19:00,08/28/2025,21:30,"Compare Decision Tree vs. Random Forest. Document results.","Online"
"Study Hard-Margin SVM",08/30/2025,19:00,08/30/2025,21:30,"Read ISLR Ch. 9, derive hard-margin objective. Watch StatQuest.","Online"
"Study Soft-Margin SVM",08/31/2025,19:00,08/31/2025,21:30,"Study soft-margin SVM, kernel trick. Derive dual form.","Online"
"Code Linear SVM",09/01/2025,19:00,09/01/2025,21:30,"Implement SVM with linear kernel (scikit-learn).","Online"
"Apply SVM to MNIST",09/02/2025,19:00,09/02/2025,21:30,"Apply SVM to downsampled MNIST (RBF kernel).","Online"
"Compare Kernels",09/03/2025,19:00,09/03/2025,21:30,"Compare linear vs. RBF kernels. Analyze results.","Online"
"Review SVM Math",09/04/2025,19:00,09/04/2025,21:30,"Revisit Lagrange multipliers, document findings.","Online"
"Study K-Means",09/06/2025,19:00,09/06/2025,21:30,"Read ISLR Ch. 10, derive K-Means objective. Ref: Géron Ch. 9.","Online"
"Code K-Means",09/07/2025,19:00,09/07/2025,21:30,"Implement K-Means from scratch.","Online"
"Study KNN",09/08/2025,19:00,09/08/2025,21:30,"Study KNN, distance metrics. Read Géron.","Online"
"Code KNN",09/09/2025,19:00,09/09/2025,21:30,"Implement KNN from scratch.","Online"
"Apply K-Means",09/10/2025,19:00,09/10/2025,21:30,"Apply K-Means to Mall Customers dataset.","Online"
"Apply KNN",09/11/2025,19:00,09/11/2025,21:30,"Apply KNN to Iris dataset, compare with Naive Bayes.","Online"
"Study PCA",09/13/2025,19:00,09/13/2025,21:30,"Read ISLR Ch. 10, derive PCA. Watch 3Blue1Brown 'Eigenvalues'.","Online"
"Review PCA Math",09/14/2025,19:00,09/14/2025,21:30,"Revisit PCA math, watch 3Blue1Brown.","Online"
"Code PCA",09/15/2025,19:00,09/15/2025,21:30,"Implement PCA from scratch (NumPy).","Online"
"Apply PCA to MNIST",09/16/2025,19:00,09/16/2025,21:30,"Apply PCA for MNIST visualization (scikit-learn).","Online"
"Experiment with PCA",09/17/2025,19:00,09/17/2025,21:30,"Vary number of components, analyze variance.","Online"
"Document PCA",09/18/2025,19:00,09/18/2025,21:30,"Document PCA results, summarize math.","Online"
"Study Neural Networks",09/20/2025,19:00,09/20/2025,21:30,"Read Goodfellow Ch. 6, study architecture. Ref: Fast.ai.","Online"
"Derive Backpropagation",09/21/2025,19:00,09/21/2025,21:30,"Derive backpropagation, chain rule. Watch Fast.ai.","Online"
"Code Neural Network",09/22/2025,19:00,09/22/2025,21:30,"Implement neural network from scratch (NumPy).","Online"
"Apply PyTorch",09/23/2025,19:00,09/23/2025,21:30,"Apply PyTorch to Fashion MNIST. Ref: PyTorch tutorials.","Online"
"Tune Neural Network",09/24/2025,19:00,09/24/2025,21:30,"Tune layers, activation functions in PyTorch.","Online"
"Document NN Results",09/25/2025,19:00,09/25/2025,21:30,"Document results, revisit backpropagation math.","Online"
"Study Gradient Boosting",09/27/2025,19:00,09/27/2025,21:30,"Read XGBoost docs, study boosting. Ref: Géron Ch. 7.","Online"
"Derive XGBoost",09/28/2025,19:00,09/28/2025,21:30,"Derive XGBoost objective, second-order gradients.","Online"
"Apply XGBoost",09/29/2025,19:00,09/29/2025,21:30,"Apply XGBoost to House Prices dataset (Kaggle).","Online"
"Tune XGBoost",09/30/2025,19:00,09/30/2025,21:30,"Tune learning rate, max_depth in XGBoost.","Online"
"Compare with RF",10/01/2025,19:00,10/01/2025,21:30,"Compare XGBoost vs. Random Forest on same dataset.","Online"
"Document Boosting",10/02/2025,19:00,10/02/2025,21:30,"Document results, summarize boosting math.","Online"
"Capstone: Preprocess",10/04/2025,19:00,10/04/2025,21:30,"Select Kaggle dataset, preprocess (handle missing values, encode).","Online"
"Capstone: Apply Models",10/05/2025,19:00,10/05/2025,21:30,"Apply Logistic Regression, Random Forest to dataset.","Online"
"Capstone: Apply XGBoost",10/06/2025,19:00,10/06/2025,21:30,"Apply XGBoost, evaluate with cross-validation.","Online"
"Capstone: Compare",10/07/2025,19:00,10/07/2025,21:30,"Compare model performance (metrics, plots).","Online"
"Capstone: Document",10/08/2025,19:00,10/08/2025,21:30,"Document initial results, identify best model.","Online"
"Capstone: Refine",10/09/2025,19:00,10/09/2025,21:30,"Explore feature importance, refine preprocessing.","Online"
"Capstone: Optimize",10/11/2025,19:00,10/11/2025,21:30,"Optimize models (stacking, feature engineering).","Online"
"Capstone: Finalize",10/12/2025,19:00,10/12/2025,21:30,"Finalize project, improve best model.","Online"
"Capstone: Report",10/13/2025,19:00,10/13/2025,21:30,"Write project report or create presentation.","Online"
"Review Basic Algorithms",10/14/2025,19:00,10/14/2025,21:30,"Review Linear Regression, Logistic, SVM math.","Online"
"Review Advanced Algorithms",10/15/2025,19:00,10/15/2025,21:30,"Review Neural Networks, XGBoost derivations.","Online"
"Summarize Learnings",10/16/2025,19:00,10/16/2025,21:30,"Revisit weak areas, summarize learnings.","Online"